{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Import the relevant packages\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport matplotlib.pyplot as plt\nfrom re import search\nimport sklearn as sk\n# from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset,TensorDataset, DataLoader\nimport datasets, transformers\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers import BertTokenizer, BertModel, AutoTokenizer,AutoModel\nimport logging\nimport shutil\n\nos.environ[\"WANDB_DISABLED\"] = \"true\" # Prevent training data performance logging to prevent disk overutilization and also to conserve RAM.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/' # Establishes the working directory\n    model_path = '../input/deberta-v3-large/deberta-v3-large/'  # NLP Model\n    learning_rate = 2e-5 # Determines how much the model parameters are updated during each iteration of training\n    weight_decay = 0.01 # Penalty factor added to the cross-entropy loss to prevent overfitting\n    num_fold = 5 # Number of times the model is cross-validated on random sub-samples of the training data\n    epochs = 5 # Complete passes through the training data\n    batch_size = 16 # An integer that specifies the number of samples to use in each batch during training\n    seed = 42 # Set seed for consistent reproducability","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the relevant data sets\ntrain = pd.read_csv(f\"{CFG.input_path}train.csv\")\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\ntrain = train.merge(titles, left_on='context', right_on='code')\n\n# https://www.kaggle.com/code/abhishek/phrase-matching-folds\ndef create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"fold\"] = -1\n    \n    data.loc[:, \"bins\"] = pd.cut(# bins the scores into 5 equal-width bins (as specified by the bins=5 argument) and assigns each row \n                                # to one of these bins. The resulting \"bins\" column is a categorical variable that will be used \n                                # to ensure that each fold contains roughly the same distribution of scores\n        data[\"score\"], bins=5, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    # ensures that each fold contains roughly the same distribution of values for a given categorical variable (in this case, the \"bins\" column)\n    kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'fold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = train['context'].unique() # Create a numpy array to store the distinct context identifiers\nprint(context.shape)\n# print(context)\n\nanchor = train['anchor'].unique() # Create a numpy array to store the distinct anchors\nprint(type(anchor))\nprint(\"Anchor unique values\")\nprint(anchor.shape)\n\ntarget = train['target'].unique() # Create a numpy array to store the distinct targets\nprint(target.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"big_anchor = max(anchor, key = len) # Find the biggest string in the 'anchor' array\nshort_anchor = min(anchor, key = len) # Find the shortest string in the 'anchor' array\n#median_string = statistics.median(len(anchor))\n#avg_string = mean(anchor, key = len)\n \n# printing results\nprint(\"\\nLongest anchor: \", big_anchor)\nprint(\"\\nLength of longest anchor:\", len(big_anchor))\nprint(\"\\nShortest anchor: \", short_anchor)\nprint(\"\\nLength of shortest anchor:\", len(short_anchor))\n# print(\"\\nMedian string: \" + median_string)\n# print(\"\\nLength of median string:\", len(median_string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"big_target = max(target, key = len) # Find the biggest string in the 'target' array\nshort_target = min(target, key = len) # Find the shortest string in the 'target' array\n\nprint(\"\\nLongest target: \", big_target)\nprint(\"\\nLength of longest target:\", len(big_target))\nprint(\"\\nShortest target: \", short_target)\nprint(\"\\nLength of shortest target:\", len(short_target))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nAnchor unique values\")\nprint(anchor.shape) # Find out the number of unique anchors\nprint(\"\\nTarget unique values\")\nprint(target.shape) # Find out the number of unique targets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['input'] = train['title']+' '+train['anchor'] # Concatenate the title and anchor columns\ntrain = create_folds(train, CFG.num_fold) # Apply the cross-validation by binning the \"score\" column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_path) # Initialize the Hugging Face Transformer by calibrating it \n                                                         # with tuned parameters from the configuration (CFG) class\nprint(\"\\n Tokenizer Activated!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        # Set the inputs to be the 'input' column of the dataframe as a numpy array\n        self.inputs = df['input'].values.astype(str)\n        # Set the targets to be the 'target' column of the dataframe as a numpy array\n        self.targets = df['target'].values.astype(str)\n        # Set the label to be the 'score' column of the dataframe as a numpy array\n        self.label = df['score'].values\n\n    def __len__(self):\n        # Return the number of inputs in the dataset\n        return len(self.inputs)\n\n    def __getitem__(self, item):\n        # Get the inputs for the given item\n        inputs = self.inputs[item]\n        # Get the targets for the given item\n        targets = self.targets[item]\n        # Get the label for the given item\n        label = self.label[item]\n        \n        # Use the tokenizer to encode the inputs and targets\n        # The ** operator is used to unpack the resulting dictionary\n        return {\n            **tokenizer( inputs, targets ),\n            # Add the label as a key in the dictionary with a float32 datatype\n            'label':label.astype(np.float32)\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    # extract predictions and labels from the input\n    predictions, labels = eval_pred\n    # reshape predictions to be of length equal to the number of predictions\n    predictions = predictions.reshape(len(predictions))\n    # compute Pearson correlation coefficient between predictions and labels\n    pearson_corr = np.corrcoef(predictions, labels)[0][1]\n    # return dictionary containing the computed metrics\n    return {'pearson': pearson_corr}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an empty dataframe to store the final predictions\nsample_sub = pd.DataFrame()\n\n# iterate over each fold\nfor fold in range(CFG.num_fold):\n    \n    # split the data into training and validation sets for the current fold\n    tr_data = train[train['fold']!=fold].reset_index(drop=True)\n    va_data = train[train['fold']==fold].reset_index(drop=True)\n    \n    # create training and validation datasets using the TrainDataset class defined earlier\n    tr_dataset = TrainDataset(tr_data)\n    va_dataset = TrainDataset(va_data)\n    \n    # set the training arguments for the Trainer class\n    args = TrainingArguments(\n        output_dir=f\"/tmp/uspppm\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=CFG.learning_rate,\n        per_device_train_batch_size=CFG.batch_size,\n        per_device_eval_batch_size=CFG.batch_size,\n        num_train_epochs=CFG.epochs,\n        weight_decay=CFG.weight_decay,\n        metric_for_best_model=\"pearson\",\n        load_best_model_at_end=True,\n    )\n    \n    # initialize the model for sequence classification and create a Trainer instance\n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=tr_dataset,\n        eval_dataset=va_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    \n    # train the model\n    trainer.train()\n    \n    # remove the temporary directory created during training\n    shutil.rmtree(f\"/tmp/uspppm\")\n    \n    # save the model weights for the current fold\n    trainer.save_model(f\"uspppm_{fold}\")\n    \n    # generate predictions on the validation set using the trained model\n    outputs = trainer.predict(va_dataset)\n    predictions = outputs.predictions.reshape(-1)\n    \n    # add the predictions to the validation data and store it in the sample_sub dataframe\n    va_data['preds'] = predictions\n    sample_sub = pd.concat([sample_sub, va_data])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract predictions and labels from the sample submission dataframe\npredictions = sample_sub['preds'].values\nlabel = sample_sub['score'].values\n\n# Create a tuple with predictions and labels\neval_pred = predictions, label\n\n# Call the `compute_metrics` function to compute the Pearson correlation coefficient between the predictions and the labels\ncompute_metrics(eval_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv('sample_sub.csv') # Compile the trained output in the competition prescribed format","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}